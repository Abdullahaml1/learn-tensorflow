{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c411aab7",
   "metadata": {},
   "source": [
    "# Multiclass Text classification [link](https://www.tensorflow.org/tutorials/keras/text_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e254c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1684576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downlaoding the dataset\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\"\n",
    "\n",
    "os.chdir('3-text-classifcation-dataset')\n",
    "dataset = tf.keras.utils.get_file(\"stack_overflow_16k\", url,\n",
    "                                    untar=True, cache_dir='.',\n",
    "                                    cache_subdir='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c75b76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 6400 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-19 01:04:50.627832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-19 01:04:50.636000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-19 01:04:50.636447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-19 01:04:50.637353: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-19 01:04:50.637949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-19 01:04:50.638353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-19 01:04:50.638695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-19 01:04:51.368408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-19 01:04:51.368740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-19 01:04:51.369006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-19 01:04:51.369223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2638 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 1600 files for validation.\n",
      "Found 8000 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# spliting train and test\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'train', \n",
    "    batch_size=BATCH_SIZE, \n",
    "    validation_split=0.2, \n",
    "    subset='training', \n",
    "    seed=SEED)\n",
    "\n",
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'train', \n",
    "    batch_size=BATCH_SIZE, \n",
    "    validation_split=0.2, \n",
    "    subset='validation', \n",
    "    seed=SEED)\n",
    "\n",
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'test', \n",
    "    batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4ef31fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparig traing data (converting it to one hot encoding)\n",
    "MAX_FEATURE = 10000\n",
    "MAX_LENTH = 250\n",
    "\n",
    "def custom_standarization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    stripped_html = tf.strings.regex_replace(stripped_html, '\\n', '')\n",
    "    return tf.strings.regex_replace(stripped_html, # removing punctuation\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')\n",
    "# x = next(iter(raw_train_ds)) # tupple (text->(batch), class)\n",
    "# print(custom_standarization(x[0][0]))\n",
    "\n",
    "\n",
    "# vectorization layer (convertinig text to one-hot-encoding)\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standarization,\n",
    "    max_tokens=MAX_FEATURE, # the lenth of the one-hot vector\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LENTH #the max number of tokens in the input sentence\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "    '''\n",
    "    this function will be mapped (applyed to the dataset) as a prameter\n",
    "    '''\n",
    "    tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "# Initializing the vocabulary of the TextVectorizaion layer\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)\n",
    "\n",
    "\n",
    "# Applying the vectorization\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c69c86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "x = next(iter(raw_train_ds)) # tupple (text->(batch), class)\n",
    "print(x[1][0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b35405b",
   "metadata": {},
   "source": [
    "### Configure the dataset for performance\n",
    "\n",
    "These are two important methods you should use when loading data to make sure that I/O does not become blocking.\n",
    "\n",
    "`.cache()` keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.\n",
    "\n",
    "`.prefetch()` overlaps data preprocessing and model execution while training. \n",
    "\n",
    "You can learn more about both methods, as well as how to cache data to disk in the [data performance guide](https://www.tensorflow.org/guide/data_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c36f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "168aece8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          160016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 16)          0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,084\n",
      "Trainable params: 160,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The model\n",
    "EMBEDDING_DIM = 16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(MAX_FEATURE +1, EMBEDDING_DIM), # 10000 x 16 (batch, sequence, embedding)\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(), # I thing average over embedding of the sqeunce(250 tokens)\n",
    "    layers.Dropout(0.2), \n",
    "    layers.Dense(4) # 4 classes\n",
    "    \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2c18564",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'],\n",
    "    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True) # sparse means the classes not vecotrs\n",
    "    # i.e [0, 0, 1, 0] insted they a number i.e: 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "210bbe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 2s 5ms/step - loss: 1.3777 - accuracy: 0.3480 - val_loss: 1.3674 - val_accuracy: 0.3837\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.3474 - accuracy: 0.4661 - val_loss: 1.3278 - val_accuracy: 0.5169\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.2961 - accuracy: 0.5322 - val_loss: 1.2693 - val_accuracy: 0.5813\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.2281 - accuracy: 0.5869 - val_loss: 1.1993 - val_accuracy: 0.6206\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.1530 - accuracy: 0.6378 - val_loss: 1.1260 - val_accuracy: 0.6531\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.0776 - accuracy: 0.6831 - val_loss: 1.0552 - val_accuracy: 0.6925\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.0082 - accuracy: 0.7145 - val_loss: 0.9910 - val_accuracy: 0.7231\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.9443 - accuracy: 0.7370 - val_loss: 0.9336 - val_accuracy: 0.7406\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.8839 - accuracy: 0.7555 - val_loss: 0.8836 - val_accuracy: 0.7456\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.8324 - accuracy: 0.7655 - val_loss: 0.8398 - val_accuracy: 0.7575\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "EPOCHS = 10\n",
    "history = model.fit(train_ds,\n",
    "                   validation_data = val_ds,\n",
    "                   epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efa96b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 - 1s - loss: 0.8692 - accuracy: 0.7374 - 745ms/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model\n",
    "loss, accuracy = model.evaluate(test_ds, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aacd43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
